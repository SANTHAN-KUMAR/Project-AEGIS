{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEGIS 3.0 Layer 1 Test: L1-SEM-2 Semantic Entropy\n",
    "## With LLM Integration (Gemini API / Local Llama)\n",
    "\n",
    "**Objective:** Validate semantic entropy correlates with extraction uncertainty\n",
    "\n",
    "**Metrics:**\n",
    "- Spearman ρ ≥ 0.60\n",
    "- AUC-ROC ≥ 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:30:24.882062Z",
     "iopub.status.busy": "2025-12-22T09:30:24.881357Z",
     "iopub.status.idle": "2025-12-22T09:30:28.887339Z",
     "shell.execute_reply": "2025-12-22T09:30:28.886502Z",
     "shell.execute_reply.started": "2025-12-22T09:30:24.882031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q numpy scipy scikit-learn pandas google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:30:28.889255Z",
     "iopub.status.busy": "2025-12-22T09:30:28.888957Z",
     "iopub.status.idle": "2025-12-22T09:30:29.786369Z",
     "shell.execute_reply": "2025-12-22T09:30:29.785721Z",
     "shell.execute_reply.started": "2025-12-22T09:30:28.889214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "SEEDS = [42, 123, 456, 789, 1000]\n",
    "np.random.seed(42)\n",
    "print(\"Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM Configuration\n",
    "**Option A:** Gemini API (recommended for cloud)\n",
    "**Option B:** Local Llama (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:30:29.787598Z",
     "iopub.status.busy": "2025-12-22T09:30:29.787256Z",
     "iopub.status.idle": "2025-12-22T09:30:31.796741Z",
     "shell.execute_reply": "2025-12-22T09:30:31.796012Z",
     "shell.execute_reply.started": "2025-12-22T09:30:29.787575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini API configured\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PASTE YOUR GEMINI API KEY HERE\n",
    "# ==========================================\n",
    "GEMINI_API_KEY = \"API_KEY_GOES_HERE\" \n",
    "\n",
    "USE_LLM = len(GEMINI_API_KEY) > 0\n",
    "\n",
    "if USE_LLM:\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    model = genai.GenerativeModel('gemini-3-pro-preview')\n",
    "    print(\"✓ Gemini API configured\")\n",
    "else:\n",
    "    print(\"⚠ No API key - using simulated LLM extraction\")\n",
    "    print(\"  This is acceptable for methodology validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:30:31.798614Z",
     "iopub.status.busy": "2025-12-22T09:30:31.798210Z",
     "iopub.status.idle": "2025-12-22T09:30:31.804626Z",
     "shell.execute_reply": "2025-12-22T09:30:31.803910Z",
     "shell.execute_reply.started": "2025-12-22T09:30:31.798590Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: 100 samples\n"
     ]
    }
   ],
   "source": [
    "# Test dataset with expert-assigned ambiguity ratings (1-5)\n",
    "# 1 = unambiguous, 5 = highly ambiguous\n",
    "TEST_DATA = [\n",
    "    # Rating 1: Clear medical terms\n",
    "    {\"text\": \"Patient diagnosed with hypoglycemia\", \"ambiguity\": 1},\n",
    "    {\"text\": \"Blood glucose level: 54 mg/dL\", \"ambiguity\": 1},\n",
    "    {\"text\": \"Administered 10 units of insulin\", \"ambiguity\": 1},\n",
    "    {\"text\": \"Symptoms include nausea and headache\", \"ambiguity\": 1},\n",
    "    \n",
    "    # Rating 2: Low ambiguity\n",
    "    {\"text\": \"Feeling tired after exercise\", \"ambiguity\": 2},\n",
    "    {\"text\": \"Blood sugar seems high today\", \"ambiguity\": 2},\n",
    "    {\"text\": \"Had some dizziness this morning\", \"ambiguity\": 2},\n",
    "    {\"text\": \"Stress from work affecting sleep\", \"ambiguity\": 2},\n",
    "    \n",
    "    # Rating 3: Moderate ambiguity  \n",
    "    {\"text\": \"Not feeling great today\", \"ambiguity\": 3},\n",
    "    {\"text\": \"Something feels different\", \"ambiguity\": 3},\n",
    "    {\"text\": \"Having a rough day\", \"ambiguity\": 3},\n",
    "    {\"text\": \"My body feels weird\", \"ambiguity\": 3},\n",
    "    \n",
    "    # Rating 4: High ambiguity\n",
    "    {\"text\": \"Just feeling off\", \"ambiguity\": 4},\n",
    "    {\"text\": \"Something is wrong\", \"ambiguity\": 4},\n",
    "    {\"text\": \"Not myself today\", \"ambiguity\": 4},\n",
    "    {\"text\": \"Things are different somehow\", \"ambiguity\": 4},\n",
    "    \n",
    "    # Rating 5: Very high ambiguity\n",
    "    {\"text\": \"Meh\", \"ambiguity\": 5},\n",
    "    {\"text\": \"Ugh\", \"ambiguity\": 5},\n",
    "    {\"text\": \"Whatever\", \"ambiguity\": 5},\n",
    "    {\"text\": \"...\", \"ambiguity\": 5},\n",
    "]\n",
    "\n",
    "# Expand to N=100 samples\n",
    "EXPANDED_DATA = TEST_DATA * 5\n",
    "print(f\"Test dataset: {len(EXPANDED_DATA)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Entropy Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:30:31.806009Z",
     "iopub.status.busy": "2025-12-22T09:30:31.805591Z",
     "iopub.status.idle": "2025-12-22T09:30:31.849570Z",
     "shell.execute_reply": "2025-12-22T09:30:31.849024Z",
     "shell.execute_reply.started": "2025-12-22T09:30:31.805960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SNOMED-CT mapping for concept clustering\n",
    "CONCEPT_MAPPING = {\n",
    "    \"hypoglycemia\": \"302866003\", \"hyperglycemia\": \"80394007\",\n",
    "    \"blood glucose\": \"33747003\", \"glucose\": \"33747003\",\n",
    "    \"insulin\": \"412222008\", \"fatigue\": \"84229001\",\n",
    "    \"tired\": \"84229001\", \"headache\": \"25064002\",\n",
    "    \"nausea\": \"422587007\", \"dizziness\": \"404640003\",\n",
    "    \"stress\": \"73595000\", \"anxiety\": \"48694002\",\n",
    "    \"malaise\": \"367391008\", \"unwell\": \"367391008\",\n",
    "    \"unknown\": \"261665006\",\n",
    "}\n",
    "\n",
    "def extract_concept_simulated(text: str, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Simulated LLM extraction with temperature-based stochasticity\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Find matching concepts\n",
    "    matches = [k for k in CONCEPT_MAPPING.keys() if k in text_lower]\n",
    "    \n",
    "    if matches:\n",
    "        if np.random.random() > temperature * 0.3:\n",
    "            return matches[0]\n",
    "        else:\n",
    "            # Temperature-based diversity\n",
    "            all_concepts = list(CONCEPT_MAPPING.keys())\n",
    "            return np.random.choice(all_concepts)\n",
    "    else:\n",
    "        # Ambiguous text - high variance\n",
    "        if np.random.random() < temperature:\n",
    "            return np.random.choice(list(CONCEPT_MAPPING.keys()))\n",
    "        return \"unknown\"\n",
    "\n",
    "def extract_concept_llm(text: str, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Real LLM extraction using Gemini\"\"\"\n",
    "    prompt = f\"\"\"Extract the main medical concept from this patient text. \n",
    "    Return ONLY one word from: hypoglycemia, hyperglycemia, glucose, insulin, \n",
    "    fatigue, headache, nausea, dizziness, stress, anxiety, malaise, unknown.\n",
    "    \n",
    "    Text: \"{text}\"\n",
    "    Concept:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={'temperature': temperature}\n",
    "        )\n",
    "        concept = response.text.strip().lower()\n",
    "        if concept in CONCEPT_MAPPING:\n",
    "            return concept\n",
    "        return \"unknown\"\n",
    "    except:\n",
    "        return extract_concept_simulated(text, temperature)\n",
    "\n",
    "def extract_concept(text: str, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Main extraction function - uses LLM if available\"\"\"\n",
    "    if USE_LLM:\n",
    "        return extract_concept_llm(text, temperature)\n",
    "    return extract_concept_simulated(text, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:30:31.851027Z",
     "iopub.status.busy": "2025-12-22T09:30:31.850526Z",
     "iopub.status.idle": "2025-12-22T09:30:37.627612Z",
     "shell.execute_reply": "2025-12-22T09:30:37.626754Z",
     "shell.execute_reply.started": "2025-12-22T09:30:31.850998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing entropy calculation...\n",
      "  Ambiguity=1: H=1.157 | Patient diagnosed with hypoglycemia\n",
      "  Ambiguity=1: H=1.295 | Blood glucose level: 54 mg/dL\n",
      "  Ambiguity=1: H=1.357 | Administered 10 units of insulin\n",
      "  Ambiguity=1: H=1.961 | Symptoms include nausea and headache\n"
     ]
    }
   ],
   "source": [
    "def compute_semantic_entropy(text: str, K: int = 10, \n",
    "                             temperatures: List[float] = [0.3, 0.5, 0.7, 0.9, 1.1]) -> float:\n",
    "    \"\"\"Compute semantic entropy over K candidate extractions\"\"\"\n",
    "    \n",
    "    # Generate K candidate extractions at varying temperatures\n",
    "    concepts = []\n",
    "    for _ in range(K // len(temperatures)):\n",
    "        for temp in temperatures:\n",
    "            concept = extract_concept(text, temp)\n",
    "            snomed = CONCEPT_MAPPING.get(concept, \"261665006\")\n",
    "            concepts.append(snomed)\n",
    "    \n",
    "    # Cluster by SNOMED code and compute entropy\n",
    "    counts = Counter(concepts)\n",
    "    total = sum(counts.values())\n",
    "    \n",
    "    # Shannon entropy\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        if p > 0:\n",
    "            entropy -= p * np.log2(p)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Test\n",
    "print(\"Testing entropy calculation...\")\n",
    "for sample in TEST_DATA[:4]:\n",
    "    entropy = compute_semantic_entropy(sample['text'], K=10)\n",
    "    print(f\"  Ambiguity={sample['ambiguity']}: H={entropy:.3f} | {sample['text'][:40]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:30:37.629110Z",
     "iopub.status.busy": "2025-12-22T09:30:37.628722Z",
     "iopub.status.idle": "2025-12-22T09:31:39.899465Z",
     "shell.execute_reply": "2025-12-22T09:31:39.898745Z",
     "shell.execute_reply.started": "2025-12-22T09:30:37.629084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running entropy calibration evaluation...\n",
      "(This may take a few minutes with LLM)\n",
      "\n",
      "============================================================\n",
      "L1-SEM-2: SEMANTIC ENTROPY CALIBRATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Samples: 100\n",
      "\n",
      "Spearman ρ: 0.6569 (Target: ≥0.60)\n",
      "p-value:    0.000000\n",
      "AUC-ROC:    0.7606 (Target: ≥0.80)\n",
      "\n",
      "Mean Entropy by Ambiguity Level:\n",
      "  Level 1: 0.885\n",
      "  Level 2: 1.209\n",
      "  Level 3: 2.351\n",
      "  Level 4: 2.211\n",
      "  Level 5: 2.261\n"
     ]
    }
   ],
   "source": [
    "def evaluate_entropy_calibration(test_data, K=10):\n",
    "    \"\"\"Evaluate correlation between entropy and ambiguity\"\"\"\n",
    "    entropies = []\n",
    "    ambiguities = []\n",
    "    \n",
    "    for sample in test_data:\n",
    "        entropy = compute_semantic_entropy(sample['text'], K=K)\n",
    "        entropies.append(entropy)\n",
    "        ambiguities.append(sample['ambiguity'])\n",
    "    \n",
    "    # Spearman correlation\n",
    "    rho, p_value = spearmanr(entropies, ambiguities)\n",
    "    \n",
    "    # AUC-ROC for detecting high ambiguity (≥4)\n",
    "    high_ambiguity = [1 if a >= 4 else 0 for a in ambiguities]\n",
    "    try:\n",
    "        auc = roc_auc_score(high_ambiguity, entropies)\n",
    "    except:\n",
    "        auc = 0.5\n",
    "    \n",
    "    # Mean entropy by ambiguity level\n",
    "    entropy_by_level = {}\n",
    "    for i in range(1, 6):\n",
    "        level_entropies = [e for e, a in zip(entropies, ambiguities) if a == i]\n",
    "        if level_entropies:\n",
    "            entropy_by_level[i] = np.mean(level_entropies)\n",
    "    \n",
    "    return {\n",
    "        'spearman_rho': rho,\n",
    "        'p_value': p_value,\n",
    "        'auc_roc': auc,\n",
    "        'entropy_by_level': entropy_by_level,\n",
    "        'n_samples': len(test_data)\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running entropy calibration evaluation...\")\n",
    "print(\"(This may take a few minutes with LLM)\")\n",
    "results = evaluate_entropy_calibration(EXPANDED_DATA, K=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"L1-SEM-2: SEMANTIC ENTROPY CALIBRATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSamples: {results['n_samples']}\")\n",
    "print(f\"\\nSpearman ρ: {results['spearman_rho']:.4f} (Target: ≥0.60)\")\n",
    "print(f\"p-value:    {results['p_value']:.6f}\")\n",
    "print(f\"AUC-ROC:    {results['auc_roc']:.4f} (Target: ≥0.80)\")\n",
    "print(\"\\nMean Entropy by Ambiguity Level:\")\n",
    "for level, entropy in results['entropy_by_level'].items():\n",
    "    print(f\"  Level {level}: {entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:31:39.900597Z",
     "iopub.status.busy": "2025-12-22T09:31:39.900354Z",
     "iopub.status.idle": "2025-12-22T09:36:28.715878Z",
     "shell.execute_reply": "2025-12-22T09:36:28.715166Z",
     "shell.execute_reply.started": "2025-12-22T09:31:39.900574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MULTI-SEED RESULTS\n",
      "============================================================\n",
      "Spearman ρ: 0.6484 ± 0.0476\n",
      "AUC-ROC:    0.7836 ± 0.0516\n"
     ]
    }
   ],
   "source": [
    "# Multi-seed evaluation\n",
    "def run_multi_seed(seeds=SEEDS):\n",
    "    all_results = []\n",
    "    for seed in seeds:\n",
    "        np.random.seed(seed)\n",
    "        result = evaluate_entropy_calibration(EXPANDED_DATA, K=10)\n",
    "        all_results.append(result)\n",
    "    \n",
    "    rhos = [r['spearman_rho'] for r in all_results]\n",
    "    aucs = [r['auc_roc'] for r in all_results]\n",
    "    \n",
    "    return {\n",
    "        'rho_mean': np.mean(rhos), 'rho_std': np.std(rhos),\n",
    "        'auc_mean': np.mean(aucs), 'auc_std': np.std(aucs)\n",
    "    }\n",
    "\n",
    "multi_results = run_multi_seed()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-SEED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Spearman ρ: {multi_results['rho_mean']:.4f} ± {multi_results['rho_std']:.4f}\")\n",
    "print(f\"AUC-ROC:    {multi_results['auc_mean']:.4f} ± {multi_results['auc_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T09:36:28.716946Z",
     "iopub.status.busy": "2025-12-22T09:36:28.716712Z",
     "iopub.status.idle": "2025-12-22T09:36:28.723795Z",
     "shell.execute_reply": "2025-12-22T09:36:28.723051Z",
     "shell.execute_reply.started": "2025-12-22T09:36:28.716925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST STATUS\n",
      "============================================================\n",
      "Spearman ρ: PASS ✓\n",
      "AUC-ROC:    FAIL ✗\n",
      "\n",
      "OVERALL: FAIL ✗\n",
      "\n",
      "Results JSON:\n",
      "{\n",
      "  \"test_id\": \"L1-SEM-2\",\n",
      "  \"test_name\": \"Semantic Entropy Calibration\",\n",
      "  \"llm_used\": true,\n",
      "  \"spearman_rho\": {\n",
      "    \"mean\": 0.6483558731856656,\n",
      "    \"std\": 0.04757772139300616\n",
      "  },\n",
      "  \"auc_roc\": {\n",
      "    \"mean\": 0.783625,\n",
      "    \"std\": 0.051636180521886836\n",
      "  },\n",
      "  \"passed\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Pass/Fail determination\n",
    "TARGETS = {'spearman_rho': 0.60, 'auc_roc': 0.80}\n",
    "\n",
    "passed = (\n",
    "    multi_results['rho_mean'] >= TARGETS['spearman_rho'] and\n",
    "    multi_results['auc_mean'] >= TARGETS['auc_roc']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST STATUS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Spearman ρ: {'PASS ✓' if multi_results['rho_mean'] >= TARGETS['spearman_rho'] else 'FAIL ✗'}\")\n",
    "print(f\"AUC-ROC:    {'PASS ✓' if multi_results['auc_mean'] >= TARGETS['auc_roc'] else 'FAIL ✗'}\")\n",
    "print(f\"\\nOVERALL: {'PASS ✓' if passed else 'FAIL ✗'}\")\n",
    "\n",
    "# Save results - convert numpy types to Python native types for JSON serialization\n",
    "final_results = {\n",
    "    'test_id': 'L1-SEM-2',\n",
    "    'test_name': 'Semantic Entropy Calibration',\n",
    "    'llm_used': bool(USE_LLM),\n",
    "    'spearman_rho': {'mean': float(multi_results['rho_mean']), 'std': float(multi_results['rho_std'])},\n",
    "    'auc_roc': {'mean': float(multi_results['auc_mean']), 'std': float(multi_results['auc_std'])},\n",
    "    'passed': bool(passed)\n",
    "}\n",
    "print(\"\\nResults JSON:\")\n",
    "print(json.dumps(final_results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
