AEGIS 3.0 — Full Testing Implementation Plan (v4 Final)
Changes: v3 → v4
Fix	What Changed
Phase 0 expanded	+T0.2 (30-patient load), +T0.3 (Bergman param cross-check)
T2.1b concrete specs	Perturbation functions defined (dawn, exercise, progressive ISF)
CL.2 MPC compute	Split into CL.2a (PID 15K MC) + CL.2b (MPC 500 MC)
Failure protocol	Blocking / non-blocking / paper-scoping classification
Failure Protocol
BLOCKING (stop downstream):
  T0.2/T0.3 — simglucose adapter broken
  T2.11    — calibration fails → L5 Seldonian is void
  T2.1a    — UDE hurts performance → basic competence
NON-BLOCKING (continue, fix later):
  T2.1b, T3.3b, T4.7, T4.8, T2.12, T3.10, T4.10
PAPER-SCOPING (adjust claims):
  T3.7 fails  → remove "detects individual effects"
  T4.2 fails  → remove √T regret claim
  CL.1b fails → remove "safe dosing" language
  T2.1b fails → remove "universal" from UDE
Current State
Layer	Status
L5	✅ Done (41 pytest + 8 sim)
L1-L4	❌ Kaggle notebooks only
Order: Phase 0 → L1 → L2 → L3 → L4 → Cross-Layer

Phase 0: Infrastructure — 3 Tests
Test	What	Pass Criterion
T0.1	Deterministic reproducibility (3 runs)	Bit-identical results
T0.2	All 30 simglucose patients load + 24h sim	No crash, glucose ∈ [20, 600]
T0.3	Bergman params: JS prototype vs simglucose	p1, p2, p3, Gb, Ib differ < 5%
IMPORTANT

T0.3 prevents confusing L2 failures that look like algorithm bugs but are plumbing bugs. The JS prototype uses hardcoded ISF = 1800/TDI. If simglucose diverges, T2.1b results are meaningless.

Compute: ~30 min | Gate: BLOCKING

Layer 1: Semantic Sensorium — 8 Tests
Files: aegis_core/semantic_sensorium.py, layer1/test_semantic_sensorium.py

Test	What	Runs	Pass Criterion
T1.1	Concept extraction (held-out)	500	F1 ≥ 0.80
T1.2	Semantic entropy calibration	200	ρ ≥ 0.60, AUC ≥ 0.80
T1.3	HITL trigger	200	Capture ≥ 85%, FAR ≤ 50%
T1.4	Non-circular proxy (held-out)	1,000	P ≥ 0.80, R ≥ 0.75
T1.5	Bias reduction grid	8,000 MC	≥ 30% vs no-proxy
T1.6	Text noise robustness	2,000	F1 ≥ 0.70
T1.7	L1→L2 schema	100	Valid output
T1.8	Negative control (non-medical)	200	F1 ≈ 0
Compute: ~2h

Layer 2: Adaptive Digital Twin — 13 Tests
Files: aegis_core/digital_twin.py, layer2/test_digital_twin.py, layer2/run_l2_simulations.py

Test	What	Runs	Pass Criterion
T2.1a	UDE vs Mech (same-model)	15,000	RMSE ≤ 105% of mech
T2.1b	UDE vs Mech (perturbed)	15,000	RMSE ≤ 95% of mech
T2.2	Neural residual	30	Var reduction ≥ 10%
T2.3	UKF covariance	15,000	Q-ratio ≥ 1.01
T2.4	UKF↔RBPF switching	12,000	CRPS improves
T2.5	Constraints + margin	1,800	0 violations
T2.6	RBPF multimodal	12,000	CRPS ≤ UKF
T2.7	Missing data	10,500	RMSE ≤ 130% baseline
T2.8	30-min prediction	15,000	RMSE ≤ 15 mg/dL
T2.9	Noise robustness	15,000	≤ 50% degradation
T2.10	State estimation	30	RMSE ≤ 20 mg/dL
T2.11	PI calibration	15,000	90% PI covers 87-93%
T2.12	Adaptation after shift	1,000	Recovers in 48h
T2.1b perturbation specs (concrete functions):

PERTURBATIONS = {
    "dawn_phenomenon":    lambda t: 1.0 + 0.2 * np.exp(-((t%1440-360)/60)**2),
    "exercise":           lambda t: 0.7 if 600 <= (t%1440) <= 660 else 1.0,
    "progressive_change": lambda t: 1.0 + 0.3 * (t / (14*1440)),
    "meal_sensitivity":   lambda t: 1.15 if any(abs(t%1440-m)<90 for m in [420,720,1080]) else 1.0,
}
# Applied as: effective_ISF = base_ISF * perturbation(t)
# Implementation: Custom ODE wrapper around Bergman with multiplicative ISF modifier
T2.11: BLOCKING gate — if L2 uncertainty is wrong, L5 Seldonian is void.

Compute: ~50-80h (GPU)

Layer 3: Causal Inference — 11 Tests
Files: aegis_core/causal_engine.py, layer3/test_causal_engine.py, layer3/run_l3_simulations.py

Test	What	Runs	Pass Criterion
T3.1	G-estimation (5×6×3 grid)	45,000 MC	RMSE ≤ 0.10
T3.2	Double robustness (2×2)	4,000 MC	Bias < 0.05
T3.3a	Proximal inference	40,000 MC	Reduction ≥ 30% vs no-proxy
T3.3b	Weak instrument guard	5,000 MC	Bias ≤ 110% when relevance < 0.3
T3.4	Confidence sequence	100,000	Uniform coverage ≥ 93%
T3.5	Harmonic estimation	1,000 MC	Peak error ≤ 1h
T3.6	Model misspecification	5,000 MC	RMSE ≤ 2× correct
T3.7	Type I error	10,000 MC	Rejection ≤ 5%
T3.8	Power analysis	6,000 MC	80% power at N=100
T3.9	CS calibration	5,000 MC	95% CS covers 92-98%
T3.10	Confounding shift	1,000 MC	Bias < 0.15 in 7 days
Compute: ~12-15h

Layer 4: Decision Engine — 11 Tests
Files: aegis_core/decision_engine.py, layer4/test_decision_engine.py, layer4/run_l4_simulations.py

Test	What	Runs	Pass Criterion
T4.1	ACB variance reduction	2,000 MC	Ratio < 1.0
T4.2	CTS regret (per-horizon)	3,500 MC	Slope 0.4-0.6 + R(t)/√t < C ∀t
T4.3	Posterior collapse	2,500 MC	Ratio < 0.5 at ≥50% blocking
T4.4	Counterfactual quality	500 MC	Bias < 0.1, Cov ≥ 90%
T4.5	CTS vs baselines	2,000 MC	CTS Pareto-dominates
T4.6	Safety Pareto front	5,000 MC	Monotone tradeoff
T4.7	Non-stationary	1,500 MC	Regret ≤ 2× stationary
T4.8	Scaling with arms	2,000 MC	Regret ≤ √K
T4.9	Posterior calibration	500 MC	90% PI covers 87-93%
T4.10	Reward shift	1,500 MC	Regret ≤ 2× in 500 steps
T4.11	Negative control	500 MC	Regret ≈ 0
Compute: ~6h

Cross-Layer — 8 Tests
Files: integration/test_integration.py

Test	What	Runs	Pass Criterion
CL.1a	TBR root-cause ablation	30 × 7d	Ablation identifies ≥1 layer whose removal reduces TBR<70 by ≥5%
CL.1b	TBR fix verification	30 × 7d	TBR<70 ≤ 10%
CL.1c	TBR fix side-effects	30 × 7d	TIR drop ≤ 5%
CL.2a	AEGIS vs PID	15,000 MC	TIR ≥ PID
CL.2b	AEGIS vs MPC	500 MC	TBR<54 ≤ MPC
CL.3	Clinical scenarios	5 × 7d	No severe hypo
CL.4	Architecture ablation	180 configs	≥ 2% marginal per layer
CL.5	L1→L3 proxy propagation	6,000 MC	E2E reduction ≥ 15%
Compute: ~30-40h (MPC reduced from 15K→500 MC)

Layer 5 — ✅ DONE (41 pytest + 8 sim)
Summary
Phase	Tests	Compute	Gate
Phase 0	3	~30 min	BLOCKING
L1	8	~2h	—
L2	13	~50-80h	T2.11 BLOCKING
L3	11	~12-15h	—
L4	11	~6h	—
L5	41 ✅	Done	—
Cross	8	~30-40h	—
Total	54 new + 41 existing = 95	~100-140h	
Git Checkpoints
# After each layer passes:
git tag l1-validated && git push --tags
git tag l2-validated && git push --tags
# etc.
